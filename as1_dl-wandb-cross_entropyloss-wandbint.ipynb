{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "878c37d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.datasets import fashion_mnist\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import pprint\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7669051c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: as1_dl (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef4dd4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_split(batch_size,X_train,Y_train,X_test,Y_test): #Split the data into batches\n",
    "    x_train, y_train_cat = shuffle(X_train,Y_train)\n",
    "    x_test,y_test=shuffle(X_test,Y_test)\n",
    "    x_train = x_train.reshape(x_train.shape[0],784)\n",
    "    x_test = x_test.reshape(x_test.shape[0],784)\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train_cat, test_size=0.1)\n",
    "    x_train=x_train/np.max(x_train)\n",
    "    x_train_raw=x_train\n",
    "    x_test=x_test/np.max(x_test)\n",
    "    x_valid=x_valid/np.max(x_valid)\n",
    "    #y_train=y_train.reshape(len(y_train), 1)\n",
    "    enc=OneHotEncoder(sparse=False)\n",
    "    y_train_enc = enc.fit_transform(np.array(y_train.reshape(len(y_train), 1)))\n",
    "    y_test_enc = enc.fit_transform(np.array(y_test.reshape(len(y_test), 1)))\n",
    "    y_valid_enc = enc.fit_transform(np.array(y_valid.reshape(len(y_valid), 1)))\n",
    "    y_train_bs=[]\n",
    "    x_train_bs=[]\n",
    "    if x_train.shape[0]%batch_size==0: #When all the batch sizes can be equal\n",
    "        x_train_bs=np.vsplit(x_train,int(x_train.shape[0]/batch_size))\n",
    "        y_train_bs=np.vsplit(y_train_enc,int(x_train.shape[0]/batch_size))\n",
    "    else: #When all the batch sizes except one are be equal\n",
    "        x_train_bs=np.vsplit(x_train[0:x_train.shape[0]-x_train.shape[0]%batch_size],math.floor(x_train[0:x_train.shape[0]-x_train.shape[0]%batch_size].shape[0]/batch_size))\n",
    "        x_train_bs.append(x_train[x_train.shape[0]-x_train.shape[0]%batch_size:x_train.shape[0]])\n",
    "        y_train_bs=np.vsplit(y_train_enc[0:x_train.shape[0]-x_train.shape[0]%batch_size],math.floor(x_train[0:x_train.shape[0]-x_train.shape[0]%batch_size].shape[0]/batch_size))\n",
    "        y_train_bs.append(y_train_enc[x_train.shape[0]-x_train.shape[0]%batch_size:x_train.shape[0]])\n",
    "    return x_train_raw,x_train_bs,y_train,y_train_bs,y_train_enc,x_test,y_test,y_test_enc,x_valid,y_valid,y_valid_enc\n",
    "\n",
    "def activ_func(x,act_func):  #activation function \n",
    "    if act_func=='logistic':\n",
    "        log_func =1 / (1 + np.exp(-x))\n",
    "        return log_func\n",
    "    elif act_func=='tanh':\n",
    "        return np.tanh(x)\n",
    "    elif act_func=='relu':\n",
    "        return np.maximum(x, 0)\n",
    "    \n",
    "def d_activ_func(x,act_func): #derivative of the activation function\n",
    "    if act_func=='logistic':\n",
    "        return activ_func(x,act_func)*(1-activ_func(x,act_func))\n",
    "    elif act_func=='tanh':\n",
    "        return 1-np.square(activ_func(x,act_func))\n",
    "    elif act_func=='relu':\n",
    "        return (x > 0) * 1\n",
    "\n",
    "def func_softmax(x): #output function as softmax\n",
    "    sf=[]\n",
    "    e_x = np.exp(x)\n",
    "    for i in range(x.shape[1]):\n",
    "        sf.append(e_x[:,i]/np.sum(e_x[:,i]))\n",
    "    return sf\n",
    "\n",
    "def compute_loss(y_predicted,y,w,loss_func,lam): \n",
    "    samp=len(y)\n",
    "    loss=0\n",
    "    loss_w=0 #loss from the regularisation\n",
    "    for i in range(len(w)):\n",
    "        loss_w=loss_w+(lam*(np.sum(np.square(w[i])))/(2*samp))\n",
    "    if loss_func=='cross_entropy':\n",
    "        for i in range(samp):\n",
    "            loss=loss+np.divide(-np.dot(np.log(y_predicted[i]),np.transpose(y[i])),samp)\n",
    "    elif loss_func=='sq_error':    \n",
    "        for i in range(samp):\n",
    "            loss=loss+np.divide(np.sum(np.square(y_predicted[i]-y[i])),samp)\n",
    "    return loss+loss_w\n",
    "\n",
    "def initialize_params(no_hidden,size_hidden,spec): #initialises weights and biases\n",
    "    mf=1e-2 #using this multiplication factor gives a much better performance. standard random weights are just too damn big\n",
    "    input_size=784\n",
    "    no_output=10\n",
    "    if spec=='xv':    #xavier initialisation\n",
    "        w=[]\n",
    "        w.append(mf*np.sqrt(1./size_hidden[0])*np.random.rand(size_hidden[0],input_size))\n",
    "        b=[]\n",
    "        b.append(0*np.random.rand(size_hidden[0],1))\n",
    "        for i in range(no_hidden-1):\n",
    "            w.append(mf*np.sqrt(1./size_hidden[i+1])*np.random.rand(size_hidden[i+1],size_hidden[i]))\n",
    "            b.append(0*np.random.rand(size_hidden[i+1],1)) \n",
    "        w.append(mf*np.sqrt(1./no_output)*np.random.rand(no_output,size_hidden[-1]))\n",
    "        b.append(0*np.random.rand(no_output,1))\n",
    "        return w,b\n",
    "    \n",
    "    elif spec=='rand':    #random initialisation\n",
    "        w=[]\n",
    "        w.append(mf*np.random.rand(size_hidden[0],input_size))\n",
    "        b=[]\n",
    "        b.append(mf*np.random.rand(size_hidden[0],1))\n",
    "        for i in range(no_hidden-1):\n",
    "            w.append(mf*np.random.rand(size_hidden[i+1],size_hidden[i]))\n",
    "            b.append(mf*np.random.rand(size_hidden[i+1],1)) \n",
    "        w.append(mf*np.random.rand(no_output,size_hidden[-1]))\n",
    "        b.append(mf*np.random.rand(no_output,1))\n",
    "        return w,b\n",
    "    \n",
    "def forward_prop(x,w,b,act_func,no_hidden): #forward propogation\n",
    "    a=[] #pre activation\n",
    "    h=[] #activation\n",
    "    h.append(x.T)\n",
    "    for i in range(no_hidden):\n",
    "        a.append(np.dot(w[i],h[i])+b[i])\n",
    "        h.append(activ_func(a[i],act_func))\n",
    "    a_f=np.dot(w[no_hidden],h[no_hidden])+b[no_hidden]\n",
    "    y_pred=func_softmax(a_f)\n",
    "    return a,h,a_f,y_pred\n",
    "\n",
    "def backward_prop(y,y_pred,w,b,h,a,a_f,no_hidden,lam,act_func): #back propogation\n",
    "    grad_a=[None]*(no_hidden+1)#gradient of the pre-activation function \n",
    "    grad_w=[None]*(no_hidden+1)#gradient of the weights\n",
    "    grad_b=[None]*(no_hidden+1)#gradient of the biases\n",
    "    grad_h=[None]*(no_hidden)# gradient of the activation function\n",
    "    grad_a[no_hidden]=-(y-y_pred)\n",
    "    num=len(y)\n",
    "    for k in range(no_hidden,-1,-1):\n",
    "        grad_w[k]=np.divide(np.dot(h[k],grad_a[k]),num)+np.divide((lam*w[k].T),num)#gradient of the weights\n",
    "        grad_b[k]= np.divide(np.sum(grad_a[k], axis=0, keepdims=True),num) #gradient of the biases\n",
    "        if k >= 1:\n",
    "            grad_h[k-1]=np.dot(grad_a[k],w[k])\n",
    "            grad_a[k-1]=grad_h[k-1]*d_activ_func(a[k-1].T,act_func)\n",
    "    return grad_b,grad_w \n",
    "\n",
    "def sg_mb_update(w,b,grad_w,grad_b,learning_rate,no_hidden): #minibatch gradient descent\n",
    "    for i in range(no_hidden+1):\n",
    "        w[i]=w[i]-(learning_rate*grad_w[i].T)\n",
    "        b[i]=b[i]-(learning_rate*grad_b[i].T)\n",
    "    return w,b\n",
    "\n",
    "def momentum_update(w,b,grad_w,grad_b,learning_rate,update_w,update_b,no_hidden): #momentum based graddient descent\n",
    "    sum_w=[i * 0 for i in w]\n",
    "    sum_b=[i * 0 for i in b]\n",
    "    gamma=0.9\n",
    "    for i in range(no_hidden+1):\n",
    "        sum_w[i]=gamma*update_w[i]+learning_rate*grad_w[i].T\n",
    "        sum_b[i]=gamma*update_b[i]+learning_rate*grad_b[i].T\n",
    "        w[i]=w[i]-sum_w[i]\n",
    "        b[i]=b[i]-sum_b[i]\n",
    "    update_w=sum_w\n",
    "    update_b=sum_b\n",
    "    return w,b,update_w,update_b\n",
    "\n",
    "def nag_lookahead(w,b,learning_rate,update_w,update_b,no_hidden): #estimate the gradients of the lookahead point in NAG\n",
    "    for i in range(no_hidden+1):\n",
    "        w[i]=w[i]-learning_rate*update_w[i]\n",
    "        b[i]=b[i]-learning_rate*update_b[i]\n",
    "    return w,b\n",
    "\n",
    "def nag_update(w,b,grad_w,grad_b,learning_rate,update_w,update_b,no_hidden): #final gradient update in NAG\n",
    "    gamma=0.9\n",
    "    sum_w=[i * 0 for i in w]\n",
    "    sum_b=[i * 0 for i in b]\n",
    "    for i in range(no_hidden+1):\n",
    "        sum_w[i]=gamma*update_w[i]+learning_rate*grad_w[i].T\n",
    "        sum_b[i]=gamma*update_b[i]+learning_rate*grad_b[i].T\n",
    "        w[i]=w[i]-sum_w[i]\n",
    "        b[i]=b[i]-sum_b[i]\n",
    "    update_w=sum_w\n",
    "    update_b=sum_b\n",
    "    return w,b,update_w,update_b\n",
    "\n",
    "def rmsprop_update(w,b,grad_w,grad_b,learning_rate,v_w,v_b,no_hidden): #RMSprop\n",
    "    beta=0.9\n",
    "    epsilon=1e-8\n",
    "    sum_w=[i * 0 for i in w]\n",
    "    sum_b=[i * 0 for i in b]\n",
    "    for i in range(no_hidden+1):\n",
    "        sum_w[i]=beta*v_w[i]+(1-beta)*(np.square(grad_w[i].T))\n",
    "        sum_b[i]=beta*v_b[i]+(1-beta)*(np.square(grad_b[i].T))\n",
    "        w[i]=w[i]-learning_rate*np.divide(grad_w[i].T,np.sqrt(sum_w[i]+epsilon))\n",
    "        b[i]=b[i]-learning_rate*np.divide(grad_b[i].T,np.sqrt(sum_b[i]+epsilon))\n",
    "    v_w=sum_w\n",
    "    v_b=sum_b\n",
    "    return w,b,v_w,v_b\n",
    "\n",
    "def adam_update(w,b,grad_w,grad_b,learning_rate,v_w,v_b,m_w,m_b,ct,no_hidden): #Adam optimisation\n",
    "    beta_1=0.9\n",
    "    beta_2=0.99\n",
    "    epsilon=1e-8\n",
    "    sum_w_m=[i * 0 for i in w]\n",
    "    sum_w_v=[i * 0 for i in w]\n",
    "    sum_b_m=[i * 0 for i in b]\n",
    "    sum_b_v=[i * 0 for i in b]\n",
    "    m_cap_w=[i * 0 for i in w]\n",
    "    m_cap_b=[i * 0 for i in b]\n",
    "    v_cap_w=[i * 0 for i in w]\n",
    "    v_cap_b=[i * 0 for i in b]\n",
    "    for i in range(no_hidden+1):\n",
    "        sum_w_m[i]=beta_1*m_w[i]+(1-beta_1)*grad_w[i].T\n",
    "        sum_w_v[i]=beta_2*v_w[i]+(1-beta_2)*(np.square(grad_w[i].T))\n",
    "\n",
    "        sum_b_m[i]=beta_1*m_b[i]+(1-beta_1)*grad_b[i].T\n",
    "        sum_b_v[i]=beta_2*v_b[i]+(1-beta_2)*(np.square(grad_b[i].T))\n",
    "\n",
    "        m_cap_w[i]=np.divide(sum_w_m[i],(1-math.pow(beta_1,ct)))\n",
    "        v_cap_w[i]=np.divide(sum_w_v[i],(1-math.pow(beta_2,ct)))\n",
    "\n",
    "        m_cap_b=np.divide(sum_b_m[i],(1-math.pow(beta_1,ct)))\n",
    "        v_cap_b=np.divide(sum_b_v[i],(1-math.pow(beta_2,ct)))\n",
    "\n",
    "        w[i]=w[i]-(learning_rate*np.divide(m_cap_w[i],np.sqrt(v_cap_w[i]+epsilon)))\n",
    "        b[i]=b[i]-(learning_rate*np.divide(m_cap_b[i],np.sqrt(v_cap_b[i]+epsilon)))\n",
    "    m_w=sum_w_m\n",
    "    m_b=sum_b_m\n",
    "    v_w=sum_w_v\n",
    "    v_b=sum_b_v\n",
    "    return w,b,v_w,v_b,m_w,m_b\n",
    "def nadam_update(w,b,grad_w,grad_b,learning_rate,v_w,v_b,m_w,m_b,ct,no_hidden): #nadam optimisation\n",
    "    beta_1=0.9\n",
    "    beta_2=0.99\n",
    "    epsilon=1e-8\n",
    "    sum_w_m=[i * 0 for i in w]\n",
    "    sum_w_v=[i * 0 for i in w]\n",
    "    sum_b_m=[i * 0 for i in b]\n",
    "    sum_b_v=[i * 0 for i in b]\n",
    "    m_cap_w=[i * 0 for i in w]\n",
    "    m_cap_b=[i * 0 for i in b]\n",
    "    v_cap_w=[i * 0 for i in w]\n",
    "    v_cap_b=[i * 0 for i in b]\n",
    "    for i in range(no_hidden+1):\n",
    "        sum_w_m[i]=beta_1*m_w[i]+(1-beta_1)*grad_w[i].T\n",
    "        sum_w_v[i]=beta_2*v_w[i]+(1-beta_2)*(np.square(grad_w[i].T))\n",
    "\n",
    "        sum_b_m[i]=beta_1*m_b[i]+(1-beta_1)*grad_b[i].T\n",
    "        sum_b_v[i]=beta_2*v_b[i]+(1-beta_2)*(np.square(grad_b[i].T))\n",
    "\n",
    "        m_cap_w[i]=np.divide(beta_1*sum_w_m[i],(1-math.pow(beta_1,ct)))+np.divide((1-beta_1)*grad_w[i].T,(1-math.pow(beta_1,ct)))\n",
    "        v_cap_w[i]=np.divide(sum_w_v[i],(1-math.pow(beta_2,ct)))\n",
    "\n",
    "        m_cap_b[i]=np.divide(beta_1*sum_b_m[i],(1-math.pow(beta_1,ct)))+np.divide((1-beta_1)*grad_b[i].T,(1-math.pow(beta_1,ct)))\n",
    "        v_cap_b[i]=np.divide(sum_b_v[i],(1-math.pow(beta_2,ct)))\n",
    "\n",
    "        w[i]=w[i]-(learning_rate*np.divide(m_cap_w[i],np.sqrt(v_cap_w[i]+epsilon)))\n",
    "        b[i]=b[i]-(learning_rate*np.divide(m_cap_b[i],np.sqrt(v_cap_b[i]+epsilon)))\n",
    "    m_w=sum_w_m\n",
    "    m_b=sum_b_m\n",
    "    v_w=sum_w_v\n",
    "    v_b=sum_b_v\n",
    "    return w,b,v_w,v_b,m_w,m_b\n",
    "\n",
    "def test_model(w,b,x_test,y_test,y_test_enc,act_func,no_hidden,loss_func,lam):  #test the parameters onn a given dataset\n",
    "    an=[]\n",
    "    hn=[]\n",
    "    hn.append(x_test.T)\n",
    "    for i in range(no_hidden):\n",
    "        an.append(np.dot(w[i],hn[i])+b[i])\n",
    "        hn.append(activ_func(an[i],act_func))\n",
    "    a_fn=np.dot(w[no_hidden],hn[no_hidden])+b[no_hidden]\n",
    "    y_pred1=func_softmax(a_fn)\n",
    "    y_final=np.empty(x_test.shape[0])\n",
    "    for i in range(x_test.shape[0]):\n",
    "        y_final[i]=y_pred1[i].argmax()\n",
    "    loss= compute_loss(y_pred1,y_test_enc,w,loss_func,lam)\n",
    "    return round(accuracy_score(y_test, y_final),4),round(loss,4)\n",
    "\n",
    "def train_model(no_hidden,size_hidden,bs,max_iterations,learning_rate,learn_algo,lam,spec,act_func,loss_func): #execute this to train your data\n",
    "#no_hidden- Number of hidden layers,size_hidden - Array containing size of each layer\n",
    "#bs- batch size,max_iterations-Number of epochs,learning_rate-learning rate,\n",
    "#learn_algo- Optimisation algorithm ('sg'-stochastic gradient descent and minibatch gradient descent(specify sizes appropriately)\n",
    "#'mb'-mini_batch gradient descent,'nag'-NAG, 'rmsprop'-RMSProp,'adam'-ADAM, 'nadam'-NADAm\n",
    "#lam-L-2 regularisation parameter,spec-weights and biases initialisation(xv-Xavier and 'random'-Random),\n",
    "#act_func- activation function ('logistic'-sigmoid function, 'tanh'-tanh function, 'relu'-Relu function)\n",
    "    ct=1;\n",
    "    (X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data() #load the data\n",
    "    input_size=784 #input of 784 pixels\n",
    "    no_output=10 #10 classes\n",
    "    x_train_raw,x_train,y_train_raw,y_train,y_train_enc,x_test,y_test,y_test_enc,x_valid,y_valid,y_valid_enc=batch_split(bs,X_train,Y_train,X_test,Y_test) #split the data to training, test and validation\n",
    "    no_batch=len(x_train)\n",
    "    w,b=initialize_params(no_hidden,size_hidden,spec)\n",
    "    \n",
    "    if learn_algo=='sg':\n",
    "        while ct<=max_iterations:\n",
    "            for eg in range(no_batch):\n",
    "                a,h,a_f,y_pred=forward_prop(x_train[eg],w,b,act_func,no_hidden)\n",
    "                grad_b,grad_w=backward_prop(y_train[eg],y_pred,w,b,h,a,a_f,no_hidden,lam,act_func)\n",
    "                w,b=sg_mb_update(w,b,grad_w,grad_b,learning_rate,no_hidden)\n",
    "            valid_acc,valid_loss=test_model(w,b,x_valid,y_valid,y_valid_enc,act_func,no_hidden,loss_func,lam)\n",
    "            train_acc,train_loss=test_model(w,b,x_train_raw,y_train_raw,y_train_enc,act_func,no_hidden,loss_func,lam)\n",
    "            wandb.log({\"accuracy\":train_acc , \"val_accuracy\": valid_acc,\"val_loss\": valid_loss,\"loss\": train_loss,\"epochs\":ct})\n",
    "            #print('valid_loss',valid_loss,'Valid_Accuracy',valid_acc)\n",
    "            #print('train_loss',train_loss,'train_Accuracy',train_acc,'\\n')\n",
    "            ct+=1        \n",
    "    elif learn_algo=='mb':\n",
    "           \n",
    "            update_w= [i * 0 for i in w]\n",
    "            update_b= [i * 0 for i in b]\n",
    "            while ct<=max_iterations:\n",
    "                for eg in range(no_batch):\n",
    "                    a,h,a_f,y_pred=forward_prop(x_train[eg],w,b,act_func,no_hidden)\n",
    "                    grad_b,grad_w=backward_prop(y_train[eg],y_pred,w,b,h,a,a_f,no_hidden,lam,act_func)\n",
    "                    w,b,update_w,update_b=momentum_update(w,b,grad_w,grad_b,learning_rate,update_w,update_b,no_hidden)\n",
    "                valid_acc,valid_loss=test_model(w,b,x_valid,y_valid,y_valid_enc,act_func,no_hidden,loss_func,lam)\n",
    "                train_acc,train_loss=test_model(w,b,x_train_raw,y_train_raw,y_train_enc,act_func,no_hidden,loss_func,lam)\n",
    "                wandb.log({\"accuracy\":train_acc , \"val_accuracy\": valid_acc,\"val_loss\": valid_loss,\"loss\": train_loss,\"epochs\":ct})\n",
    "                #print('valid_loss',valid_loss,'Valid_Accuracy',valid_acc)\n",
    "                #print('train_loss',train_loss,'train_Accuracy',train_acc,'\\n')\n",
    "                ct+=1\n",
    "    elif learn_algo=='nag':  \n",
    "            update_w= [i * 0 for i in w]\n",
    "            update_b= [i * 0 for i in b]\n",
    "            while ct<=max_iterations:\n",
    "                for eg in range(no_batch):\n",
    "                    w,b=nag_lookahead(w,b,learning_rate,update_w,update_b,no_hidden)\n",
    "                    a,h,a_f,y_pred=forward_prop(x_train[eg],w,b,act_func,no_hidden)\n",
    "                    grad_b,grad_w=backward_prop(y_train[eg],y_pred,w,b,h,a,a_f,no_hidden,lam,act_func)\n",
    "                    w,b,update_w,update_b=nag_update(w,b,grad_w,grad_b,learning_rate,update_w,update_b,no_hidden)\n",
    "                valid_acc,valid_loss=test_model(w,b,x_valid,y_valid,y_valid_enc,act_func,no_hidden,loss_func,lam)\n",
    "                train_acc,train_loss=test_model(w,b,x_train_raw,y_train_raw,y_train_enc,act_func,no_hidden,loss_func,lam)\n",
    "                wandb.log({\"accuracy\":train_acc , \"val_accuracy\": valid_acc,\"val_loss\": valid_loss,\"loss\": train_loss,\"epochs\":ct})\n",
    "                #print('valid_loss',valid_loss,'Valid_Accuracy',valid_acc)\n",
    "                #print('train_loss',train_loss,'train_Accuracy',train_acc,'\\n')\n",
    "                ct+=1\n",
    "    elif learn_algo=='rmsprop':    \n",
    "            v_w= [i * 0 for i in w]\n",
    "            v_b= [i * 0 for i in b]\n",
    "            while ct<=max_iterations:\n",
    "                for eg in range(no_batch):\n",
    "                    a,h,a_f,y_pred=forward_prop(x_train[eg],w,b,act_func,no_hidden)\n",
    "                    grad_b,grad_w=backward_prop(y_train[eg],y_pred,w,b,h,a,a_f,no_hidden,lam,act_func,)\n",
    "                    w,b,v_w,v_b=rmsprop_update(w,b,grad_w,grad_b,learning_rate,v_w,v_b,no_hidden)\n",
    "                valid_acc,valid_loss=test_model(w,b,x_valid,y_valid,y_valid_enc,act_func,no_hidden,loss_func,lam)\n",
    "                train_acc,train_loss=test_model(w,b,x_train_raw,y_train_raw,y_train_enc,act_func,no_hidden,loss_func,lam)\n",
    "                wandb.log({\"accuracy\":train_acc , \"val_accuracy\": valid_acc,\"val_loss\": valid_loss,\"loss\": train_loss,\"epochs\":ct})\n",
    "                #print('valid_loss',valid_loss,'Valid_Accuracy',valid_acc)\n",
    "                #print('train_loss',train_loss,'train_Accuracy',train_acc,'\\n')\n",
    "                ct+=1\n",
    "    elif learn_algo=='adam':    \n",
    "            v_w= [i * 0 for i in w]\n",
    "            m_w= [i * 0 for i in w]\n",
    "            v_b= [i * 0 for i in b]\n",
    "            m_b= [i * 0 for i in b]\n",
    "            while ct<=max_iterations:\n",
    "                for eg in range(no_batch):\n",
    "                    a,h,a_f,y_pred=forward_prop(x_train[eg],w,b,act_func,no_hidden)\n",
    "                    grad_b,grad_w=backward_prop(y_train[eg],y_pred,w,b,h,a,a_f,no_hidden,lam,act_func)\n",
    "                    w,b,v_w,v_b,m_w,m_b=adam_update(w,b,grad_w,grad_b,learning_rate,v_w,v_b,m_w,m_b,ct,no_hidden)\n",
    "                valid_acc,valid_loss=test_model(w,b,x_valid,y_valid,y_valid_enc,act_func,no_hidden,loss_func,lam)\n",
    "                train_acc,train_loss=test_model(w,b,x_train_raw,y_train_raw,y_train_enc,act_func,no_hidden,loss_func,lam)\n",
    "                wandb.log({\"accuracy\":train_acc , \"val_accuracy\": valid_acc,\"val_loss\": valid_loss,\"loss\": train_loss,\"epochs\":ct})\n",
    "                #print('valid_loss',valid_loss,'Valid_Accuracy',valid_acc)\n",
    "                #print('train_loss',train_loss,'train_Accuracy',train_acc,'\\n')\n",
    "                ct+=1            \n",
    "    elif learn_algo=='nadam':    \n",
    "            v_w= [i * 0 for i in w]\n",
    "            m_w= [i * 0 for i in w]\n",
    "            v_b= [i * 0 for i in b]\n",
    "            m_b= [i * 0 for i in b]\n",
    "            while ct<=max_iterations:\n",
    "                for eg in range(no_batch):\n",
    "                    a,h,a_f,y_pred=forward_prop(x_train[eg],w,b,act_func,no_hidden)\n",
    "                    grad_b,grad_w=backward_prop(y_train[eg],y_pred,w,b,h,a,a_f,no_hidden,lam,act_func)\n",
    "                    w,b,v_w,v_b,m_w,m_b=nadam_update(w,b,grad_w,grad_b,learning_rate,v_w,v_b,m_w,m_b,ct,no_hidden)\n",
    "                valid_acc,valid_loss=test_model(w,b,x_valid,y_valid,y_valid_enc,act_func,no_hidden,loss_func,lam)\n",
    "                train_acc,train_loss=test_model(w,b,x_train_raw,y_train_raw,y_train_enc,act_func,no_hidden,loss_func,lam)\n",
    "                #print('valid_loss',valid_loss,'Valid_Accuracy',valid_acc)\n",
    "                #print('train_loss',train_loss,'train_Accuracy',train_acc,'\\n')\n",
    "                wandb.log({\"accuracy\":train_acc , \"val_accuracy\": valid_acc,\"val_loss\": valid_loss,\"loss\": train_loss,\"epochs\":ct})\n",
    "                ct+=1\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e96e4796",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config={'method':'grid'} #set the type of search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5996b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric={'name':'acc','goal':'maximize'}\n",
    "sweep_config['metric']=metric #set the metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8c94a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_dict={'no_hidden':{'values':[3,4]},'size_hidden':{'values':[32]},'max_iterations':{'values':[10]},\n",
    "    'bs':{'values':[64]},'learning_rate':{'values':[0.001]},'learn_algo':{'values':['sg','nadam']},'lam':{'values':[0.0005]},\n",
    "                    'spec':{'values':['rand']},'act_func':{'values':['tanh']},'loss_func':{'values':['cross_entropy']}} #all parameters neede for the sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08577d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config['parameters']=parameters_dict #add to the sweep config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3343ddb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'grid',\n",
      " 'metric': {'goal': 'maximize', 'name': 'acc'},\n",
      " 'parameters': {'act_func': {'values': ['tanh']},\n",
      "                'bs': {'values': [64]},\n",
      "                'lam': {'values': [0.0005]},\n",
      "                'learn_algo': {'values': ['sg', 'nadam']},\n",
      "                'learning_rate': {'values': [0.001]},\n",
      "                'loss_func': {'values': ['cross_entropy']},\n",
      "                'max_iterations': {'values': [10]},\n",
      "                'no_hidden': {'values': [3, 4]},\n",
      "                'size_hidden': {'values': [32]},\n",
      "                'spec': {'values': ['rand']}}}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(sweep_config) #print config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae3b42e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: m7wuu851\n",
      "Sweep URL: https://wandb.ai/as1_dl/test-project/sweeps/m7wuu851\n"
     ]
    }
   ],
   "source": [
    "sweep_id=wandb.sweep(sweep_config,project='test-project') #create sweep id and pass it to the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8834d5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Agent Starting Run: hkvxyxxi with config:\n",
      "wandb: \tact_func: tanh\n",
      "wandb: \tbs: 64\n",
      "wandb: \tlam: 0.0005\n",
      "wandb: \tlearn_algo: sg\n",
      "wandb: \tlearning_rate: 0.001\n",
      "wandb: \tloss_func: cross_entropy\n",
      "wandb: \tmax_iterations: 10\n",
      "wandb: \tno_hidden: 3\n",
      "wandb: \tsize_hidden: 32\n",
      "wandb: \tspec: rand\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/as1_dl/test-project/runs/hkvxyxxi\" target=\"_blank\">youthful-sweep-1</a></strong> to <a href=\"https://wandb.ai/as1_dl/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/as1_dl/test-project/sweeps/m7wuu851\" target=\"_blank\">https://wandb.ai/as1_dl/test-project/sweeps/m7wuu851</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 33908... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▁▁▁▁▁▁▁█▁</td></tr><tr><td>epochs</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>██▆▆▆▆▃▃▁▁</td></tr><tr><td>val_accuracy</td><td>▂▂▂▂▂▂▂▂█▁</td></tr><tr><td>val_loss</td><td>████▅▅▅▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.1006</td></tr><tr><td>epochs</td><td>10</td></tr><tr><td>loss</td><td>2.3022</td></tr><tr><td>val_accuracy</td><td>0.0942</td></tr><tr><td>val_loss</td><td>2.3023</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">youthful-sweep-1</strong>: <a href=\"https://wandb.ai/as1_dl/test-project/runs/hkvxyxxi\" target=\"_blank\">https://wandb.ai/as1_dl/test-project/runs/hkvxyxxi</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20220222_140712-hkvxyxxi\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Sweep Agent: Waiting for job.\n",
      "wandb: Job received.\n",
      "wandb: Agent Starting Run: lvglaj27 with config:\n",
      "wandb: \tact_func: tanh\n",
      "wandb: \tbs: 64\n",
      "wandb: \tlam: 0.0005\n",
      "wandb: \tlearn_algo: sg\n",
      "wandb: \tlearning_rate: 0.001\n",
      "wandb: \tloss_func: cross_entropy\n",
      "wandb: \tmax_iterations: 10\n",
      "wandb: \tno_hidden: 4\n",
      "wandb: \tsize_hidden: 32\n",
      "wandb: \tspec: rand\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/as1_dl/test-project/runs/lvglaj27\" target=\"_blank\">lucky-sweep-2</a></strong> to <a href=\"https://wandb.ai/as1_dl/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/as1_dl/test-project/sweeps/m7wuu851\" target=\"_blank\">https://wandb.ai/as1_dl/test-project/sweeps/m7wuu851</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 19188... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epochs</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.1002</td></tr><tr><td>epochs</td><td>10</td></tr><tr><td>loss</td><td>2.3026</td></tr><tr><td>val_accuracy</td><td>0.0983</td></tr><tr><td>val_loss</td><td>2.3026</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">lucky-sweep-2</strong>: <a href=\"https://wandb.ai/as1_dl/test-project/runs/lvglaj27\" target=\"_blank\">https://wandb.ai/as1_dl/test-project/runs/lvglaj27</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20220222_140755-lvglaj27\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Agent Starting Run: swar74nk with config:\n",
      "wandb: \tact_func: tanh\n",
      "wandb: \tbs: 64\n",
      "wandb: \tlam: 0.0005\n",
      "wandb: \tlearn_algo: nadam\n",
      "wandb: \tlearning_rate: 0.001\n",
      "wandb: \tloss_func: cross_entropy\n",
      "wandb: \tmax_iterations: 10\n",
      "wandb: \tno_hidden: 3\n",
      "wandb: \tsize_hidden: 32\n",
      "wandb: \tspec: rand\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/as1_dl/test-project/runs/swar74nk\" target=\"_blank\">neat-sweep-3</a></strong> to <a href=\"https://wandb.ai/as1_dl/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/as1_dl/test-project/sweeps/m7wuu851\" target=\"_blank\">https://wandb.ai/as1_dl/test-project/sweeps/m7wuu851</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 35552... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▄▆▇▇█████</td></tr><tr><td>epochs</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>█▅▃▃▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▇██████</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.8891</td></tr><tr><td>epochs</td><td>10</td></tr><tr><td>loss</td><td>0.3086</td></tr><tr><td>val_accuracy</td><td>0.8762</td></tr><tr><td>val_loss</td><td>0.3513</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">neat-sweep-3</strong>: <a href=\"https://wandb.ai/as1_dl/test-project/runs/swar74nk\" target=\"_blank\">https://wandb.ai/as1_dl/test-project/runs/swar74nk</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20220222_140833-swar74nk\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Agent Starting Run: fmvu4q1x with config:\n",
      "wandb: \tact_func: tanh\n",
      "wandb: \tbs: 64\n",
      "wandb: \tlam: 0.0005\n",
      "wandb: \tlearn_algo: nadam\n",
      "wandb: \tlearning_rate: 0.001\n",
      "wandb: \tloss_func: cross_entropy\n",
      "wandb: \tmax_iterations: 10\n",
      "wandb: \tno_hidden: 4\n",
      "wandb: \tsize_hidden: 32\n",
      "wandb: \tspec: rand\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/as1_dl/test-project/runs/fmvu4q1x\" target=\"_blank\">lunar-sweep-4</a></strong> to <a href=\"https://wandb.ai/as1_dl/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/as1_dl/test-project/sweeps/m7wuu851\" target=\"_blank\">https://wandb.ai/as1_dl/test-project/sweeps/m7wuu851</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 15408... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▄▆▇▇█████</td></tr><tr><td>epochs</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>█▅▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▇██████</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.8843</td></tr><tr><td>epochs</td><td>10</td></tr><tr><td>loss</td><td>0.3371</td></tr><tr><td>val_accuracy</td><td>0.8645</td></tr><tr><td>val_loss</td><td>0.3848</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">lunar-sweep-4</strong>: <a href=\"https://wandb.ai/as1_dl/test-project/runs/fmvu4q1x\" target=\"_blank\">https://wandb.ai/as1_dl/test-project/runs/fmvu4q1x</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20220222_140911-fmvu4q1x\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Sweep Agent: Waiting for job.\n",
      "wandb: Sweep Agent: Exiting.\n"
     ]
    }
   ],
   "source": [
    "def train2(config=None): #function that is used by wandb agent to call the training model and perform the sweep\n",
    "    with wandb.init(config=config):  # this gets over-written in the Sweep\n",
    "        size_hidden=[]\n",
    "        config = wandb.config\n",
    "        no_hidden=config.no_hidden\n",
    "        #no_hidden=4\n",
    "        for i in range(no_hidden): #loop to initialise each layer with the same number of neurons. This is done only for the sweep. Normally different neurons can also be precribed\n",
    "            size_hidden.append(config.size_hidden)\n",
    "    \n",
    "        max_iterations=config.max_iterations \n",
    "        bs=config.bs\n",
    "        learning_rate=config.learning_rate\n",
    "        learn_algo=config.learn_algo\n",
    "        lam=config.lam\n",
    "        spec=config.spec\n",
    "        act_func=config.act_func\n",
    "        loss_func=config.loss_func\n",
    "        train_model(no_hidden,size_hidden,bs,max_iterations,learning_rate,learn_algo,lam,spec,act_func,loss_func) #obtains the cross validation accuracy score\n",
    "        #wandb.log({\"Accuracy\":acc}) #stores the score for each parameter search\n",
    "\n",
    "wandb.agent(sweep_id, function=train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7851a38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
